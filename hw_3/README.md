# Homework 3 - Knapsack Algorithms: Experimental Evaluation

## Introduction

The Knapsack problem can be solved by many different algorithms, both exact and non-exact (approximate) ones. Not much is known *a priori* about their detailed properties. To uncover these properties, the algorithms must be tested experimentally.

We will observe two important parameters:

1. **Result Quality**  
2. **Computational Complexity**

These parameters will be analyzed as dependencies on various instance parameters. Proper evaluation of the observations will help determine which algorithm is the most time/quality-efficient for a given instance type.

Additionally, the **robustness** of the algorithms will be evaluated, i.e., the stability of the solution with respect to different instance encodings.

---

## Key Concepts

### Result Quality
For instances where the optimum solutions are known, computing the result quality (relative error) is straightforward. However, in this homework, new random instances will be generated. The optimum solutions can still be computed using methods like dynamic programming, even for larger instances.

### Computational Complexity
As in previous tasks, computational complexity can be measured in terms of:
- The number of visited states (tested configurations), or
- The run-time.  

Choose wisely which approach to adopt.

### Robustness
The robustness of an algorithm refers to its sensitivity to different instance encodings, in terms of result quality and run-time. For the Knapsack problem, robustness will likely involve independence from the ordering of items in the instance (e.g., in the input file).

---

## The Experiments

Every experiment is designed to answer a specific question. The data used must be representative to avoid misleading or incorrect conclusions. To enable such exploration, the instance generator is parametrized. Some algorithms may be sensitive to certain instance characteristics:

- **Dynamic Programming**: May be sensitive to maximum cost or maximum weight.
- **Knapsack Initialization**: Methods starting from a full knapsack may differ from those starting from an empty knapsack, especially for instances with varying knapsack capacity-to-total-weight ratios.
- **Granularity**: The influence of having more small items versus more large items is unclear.
- **Heuristic Methods**: Are they robust?

### Types of Experiments
1. **Pilot Experiments**:
   - Aim: To discover if a given dependency exists.
   - Conducted with a limited number of instances.
   - Results serve as a hint for subsequent experiments and need not be included in the report.

2. **Detailed Experiments**:
   - Aim: To credibly document observed dependencies.
   - Instances must be selected wisely.

---

## What Should Be Done

1. **Sensitivity Analysis**:
   - Explore the sensitivity of your algorithms to statistical parameters of the instances generated by the instance generator.
   - If other parameters are suspected to affect the algorithms, modify the generator source code and perform the necessary experiments.

2. **Tradeoff Analysis**:
   - Design and perform detailed experiments showing tradeoffs between computational complexity and result quality in solving the Knapsack problem.

3. **Robustness Analysis**:
   - Explore the robustness of the algorithms by permuting the items in instances and observing the result quality and run-time.

---

## Specific Tasks

### Algorithms to Test
- **Brute Force**:
  - Only if its implementation is not completely straightforward (e.g., always generating \(2^n\) combinations).
- **Branch & Bound**  
- **Dynamic Programming**:
  - No need to test FPTAS again since its properties are derived from DP.
- **Cost/Weight Heuristic**

### Parameters to Explore
Analyze the dependency of computational complexity (number of algorithm steps, run-time) and relative error (for heuristics) on:

1. Instance size (number of items).  
2. Maximum cost.  
3. Maximum weight.  
4. Knapsack capacity-to-total-weight ratio.  
5. Cost/weight correlation.  
6. Granularity.

### Experiment Design
- Fix all parameters of the instance generator and vary only one parameter in each measurement.
- Perform measurements on **at least four data points** (four different values of the parameter being observed). Fewer data points may obscure dependencies.

---

## Report Requirements

The report should include:

1. **Algorithm Descriptions**:
   - Provide at least a brief description of the algorithms used (e.g., DP decomposition by cost or capacity).

2. **Experimental Results**:
   - Include observations and dependencies for all tested parameters.

3. **Discussion**:
   - Analyze the tradeoffs between computational complexity and result quality.
   - Discuss the robustness of the algorithms.

---

### Notes
- Ensure the report is clear and well-structured.
- Use graphs and tables to present results effectively.